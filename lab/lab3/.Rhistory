install.packages("factoextra")
library(stats)
library(ggplot2)
library(factoextra)
#Load the data from the US_stats file
data_states = read.csv("US_States_2014.csv")
#Explore the data to get a sense of the attributes
head(data_states, n =3)
#Explore the data to get a sense of the attributes
head(data_states, n =3)
#get a summary
summary(data_states)
normalize = function(x){
return ((x - min(x))/(max(x) - min(x)))}
normalize(c(1,2,3,4,5))
library(standardize)
scale(c(1,2,3,4,5))
View(data_states)
#Let us normalize our data using the min-max function
#First, create a copy of the dataset, so we can keep the original data
data_norm <- data_states
View(data_norm)
data_norm[, 2:10] <- apply(data_norm[, 2:10], MARGIN = 2, FUN = normalize)
View(data_norm)
#If we want to use standardization, apply the function scale to the dataset
data_stand <- data_states
#Example of how scale works.
#Remember standardization will transform your attributes to have mean 0 and SD 1
scale(c(1,2,3,4,5))
data_stand[, 2:10] <- apply(data_stand[, 2:10], MARGIN = 2, FUN = scale)
View(data_stand)
View(data_norm)
#EXAMPLE 1
#Let us start implementing hierarchical clustering
#First, compute the distance matrix, using a distance measure of choice between
#euclidian, maximum and manhattan.
#The distance matrix contains the pairwise distances between points
#Let us use the euclidian distance
#The distance matrix is computed using the normalized data.
#Let us use the min-max normalization
?dist
distance_matrix_norm = dist(data_norm[, 2:10], method = "euclidean")
#If you want to see the complete distance matrix
View(as.matrix(distance_matrix_norm))
#Next, we implement the hierarchical clustering algorithm using the hclust() function
#We need to input the distance matrix and we need to specify which (dis)similarity measure
#we want to use to measure distance between clusters, among:
#single, complete, average, centroid, ward.D
#if you run help(hclust) you will see all the methods available
?hclust
h1 = hclust(distance_matrix_norm, method = "ward.D")
#To get the dendrogram, we use plot and input the hierarchical results
plot(h1, hang = -1, cex = 0.4)
plot(h1, hang = -1, cex = 0.4, labels = data_states$State)
rect.hclust(h1, k = 4, border = 2:5)
rect.hclust(h1, k = 6, border = 2:7)
?rect.hclust
#Draw clusters on the dendrogram; pick the number of clusters you want: k =
#the option border is used to pick different colors for the borders of the clusters
rect.hclust(h1, k = 4, border = 4)
rect.hclust(h1, k = 4, border = 4)
#Draw clusters on the dendrogram; pick the number of clusters you want: k =
#the option border is used to pick different colors for the borders of the clusters
rect.hclust(h1, k = 4, border = 2:5)
#Cut the dendrogram into groups
#The function cutree() can be used to get a vector containing the cluster number of each observation:
hcluster_4 <- cutree(h1, k = 4)
# Number of members/points in each cluster
table(hcluster_4)
#To see which observations belong to the 4 clusters, we can create
#a new column in our original dataset with the cluster numbers
data_states$hcluster_4 <- hcluster_4
View(data_states)
#and then we can look at the summary statistics for the different clusters
#for easy of interpretation, we usually take the summary statistics of the original, non-normalized data
summary(subset(data_states, data_states$hcluster_4 == 1))
summary(subset(data_states, data_states$hcluster_4 == 2))
#to make this process a little more efficient, we can use the package plyr
install.packages("plyr")
library(plyr)
#we use the function ddply, where we specify the original data we want to use for the summarization
#which variable we want to use to "group" our data
#which function we want to use: in this case, summarize
#which attributes we want to use and which stats we want; in this case we want the mean for Female_MedianAge and
#for Robbery; we can add more attributes if desired
?ddply
ddply(data_states, .(hcluster_4), summarize, F_MedAge=mean(Female_MedianAge), Robbery=mean(Robbery))
ddply(data_states, .(hcluster_4), summarize, F_MedAge=mean(Female_MedianAge), Robbery=mean(Robbery))
#We could also create a table to look at the distribution of the clusters across different
#categorizations of our data-points
#In this dataset, for example, the States are categorized by Regions.
#We can see the distribution of the clusters by Region
table(data_states$Region, data_states$hcluster_4)
#and we could repeat the summarization of some of the attributes by Region
ddply(data_states, .(Region), summarize, F_MedAge=mean(Female_MedianAge), Robbery=mean(Robbery))
h2 = hclust(distance_matrix_norm, method = "average")
plot(h2, labels = data_states$State)
rect.hclust(h2, k = 4, border = 2:4)
h2_4 <- cutree(h2, k = 4)
table(h2_4)
data_states$h2_4 <- h2_4
h2 = hclust(distance_matrix_norm, method = "average")
plot(h2, labels = data_states$State)
rect.hclust(h2, k = 4, border = 2:4)
h2_4 <- cutree(h2, k = 4)
table(h2_4)
data_states$h2_4 <- h2_4
set.seed(123)
k1 = kmeans(data_norm[, 2:10], centers = 4, nstart = 15)
#Look at a summary of the result
str(k1)
?kmeans
#Look at a summary of the result
str(k1)
set.seed(12)
k1 = kmeans(data_norm[, 2:10], centers = 4, nstart = 15)
#Look at a summary of the result
str(k1)
k1 = kmeans(data_norm[, 2:10], centers = 4, nstart = 15)
#Look at a summary of the result
str(k1)
#check size of each cluster
k1$size
#check cluster centroids
k1$centers
#Add a column to the original data to indicate in which cluster each observation is
data_states$k_clust <- k1$cluster
View(data_states)
#Use the original dataset to do summary statistics like we did before
ddply(data_states, .(k_clust), summarize, F_MedAge=mean(Female_MedianAge), Robbery=mean(Robbery))
fviz_cluster(k1, geom = "point", data=data_norm[, 2:10]) + ggtitle("Clusters")
#specify the clustering solution, the data used
?fviz_cluster
fviz_cluster(k1, geom = "point", data=data_norm[, 2:10], choose.vars = c("Unemployment_Rate", "Robbery")) +
ggtitle("Clusters")
fviz_cluster(k1, geom = "point", data=data_norm[, 2:10]) + ggtitle("Clusters")
fviz_cluster(k1, geom = "point", data=data_norm[, 2:10], choose.vars = c("Unemployment_Rate", "Robbery")) +
ggtitle("Clusters")
View(data_norm)
#We can also run kmeans for different number of k
#and plot the results in a grid
library(gridExtra)
k2 <- kmeans(data_norm[, 2:10], centers = 2, nstart = 15)
k3 <- kmeans(data_norm[, 2:10], centers = 3, nstart = 15)
k4 <- kmeans(data_norm[, 2:10], centers = 4, nstart = 15)
k5 <- kmeans(data_norm[, 2:10], centers = 5, nstart = 15)
p2 <- fviz_cluster(k2, geom = "point", data = data_norm[, 2:10]) + ggtitle("k = 2")
p3 <- fviz_cluster(k3, geom = "point", data = data_norm[, 2:10]) + ggtitle("k = 3")
p4 <- fviz_cluster(k4, geom = "point",  data = data_norm[, 2:10]) + ggtitle("k = 4")
p5 <- fviz_cluster(k5, geom = "point",  data = data_norm[, 2:10]) + ggtitle("k = 5")
grid.arrange(p2, p3, p4, p5, nrow = 2)
#Let us see how we can compute the WSS and create the Elbow Plot
#Create an empty vector to store the results for the WSS
WSS_curve <- c()
#in this example, we run k-means for 10 times (from 1 to 10)
#and with nstart = 5
?kmeans
?tot.withinss
k2 <- kmeans(data_norm[, 2:10], centers
= 2, nstart = 15)
k2
str(k2)
for (n in 1:10) {
k = kmeans(data_norm[,2:10], centers = n, nstart = 5)
wss = k$tot.withinss
WSS_curve[n] <- wss}
WSS_curve <- c()
BSS_curve <- c()
for (n in 1:10) {
k = kmeans(data_norm[,2:10], centers = n, nstart = 5)
wss = k$tot.withinss
bss = k$betweenss
WSS_curve[n] <- wss
BSS_curve[n] <- bss}
lines(1:10, BSS_curve,type="o",col="blue")
plot(1:10, WSS_curve, type = "b", col = "red", ylab = "WSS and BSS", xlab = "K", ylim = c(0,18)  )
lines(1:10, BSS_curve,type="o",col="blue")
#load the packages
library(stats)
library(factoextra)
#Read the data from constituents-financials_csv.csv
data <- read.csv("constituents-financials_csv.csv")
View(data)
#Summarize the dataset to get a feel of the data.
summary(data)
#----[Normalization]
#Choose the variables you should normalize, store them in a vector called var_names.
var_names <- c("Price","Dividend.Yield","Earnings.Share","X52.Week.Low","X52.Week.High","Market.Cap","EBITDA","Price.Sales")
##Now normalize the above variables using standardization
# Use the scale function to standardize your data
library(standardize)
#create a copy of the data
data_scale <- data
#apply the standardization to the attributes you included in the var_names
# margin = 2: apply to column
data_scale[, 4: 11] <- apply(data_scale[, 4:11], MARGIN = 2, fun = scale)
#apply the standardization to the attributes you included in the var_names
# margin = 2: apply to column
data_scale[, 4: 11] <- apply(data_scale[, 4:11], MARGIN = 2, FUN =  = scale)
#apply the standardization to the attributes you included in the var_names
# margin = 2: apply to column
data_scale[, 4: 11] <- apply(data_scale[, 4:11], MARGIN = 2, FUN =scale)
View(data_scale)
#Check if all the attributes you standardized have a mean 0
summary(data_scale)
data_scale2 <- data
data_scale2[, var_names] <- apply(data_scale2[, var_names], MARGIN = 2, FUN = scale)
View(data_scale2)
summary(data_scale2)
#----[Hierarchical Clustering]
# Create a dataframe data_IT with only companies in the Information Technology sector.
?subset
data_scale2$Sector
unique(data_scale2$Sector)
data_IT <- subset(data_scale2, data_scale2$Sector == "Information Technology")
data_IT
head(data_IT)
head(data_IT)
head(data_IT)
View(data_IT)
#----[Hierarchical Clustering]
# First create a dataframe data_IT with only companies in the Information Technology sector.
data.IT <- data_stand[data_stand$Sector=="Information Technology",]
View(data.IT)
sub_IT <- subset(data_stand, data_stand$Sector == "Information Technology")
View(sub_IT)
View(data_stand)
data <- read.csv("constituents-financials_csv.csv")
#Summarize the dataset to get a feel of the data.
summary(data)
#----[Normalization]
#Choose the variables you should normalize, store them in a vector called var_names.
var_names <- c("Price","Dividend.Yield","Earnings.Share","X52.Week.Low","X52.Week.High","Market.Cap","EBITDA","Price.Sales")
##Now normalize the above variables using standardization
# Use the scale function to standardize your data
library(standardize)
#create a copy of the data
data_stand <- data
#apply the standardization to the attributes you included in the var_names
data_stand[, var_names] <- apply(data_stand[, var_names], MARGIN = 2, FUN = scale)
#Check if all the attributes you standardized have a mean 0
summary(data_stand[, var_names])
#----[Hierarchical Clustering]
# First create a dataframe data_IT with only companies in the Information Technology sector.
data.IT <- data_stand[data_stand$Sector=="Information Technology",]
View(data_states)
View(data_stand)
sub_IT <- subset(data_stand, data_stand$Sector == "Information Technology")
View(sub_IT)
#----[Hierarchical Clustering]
# First create a dataframe data_IT with only companies in the Information Technology sector.
data_IT <- data_stand[data_stand$Sector=="Information Technology",]
#Rename the rows to company symbol, this should help you identify the distance matrix cells
rownames(data_IT) <- data_IT$Symbol
View(data_IT)
data.IT <- subset(data_scale2, data_scale2$Sector == "Information Technology")
View(data.IT)
#Rename the rows to company symbol, this should help you identify the distance matrix cells
rownames(data.IT) <- data.IT$Symbol
#Create a distance matrix for the newly filtered dataset using the manhattan measure.
#What variables should you use?
dist_matrix = dist(data.IT[, var_names], method = "manhattan")
#View the matrix
View(dist_matrix)
#View the matrix
View(as.matrix(dist_matrix))
# Use the hclust() to generate hierarchical clusters from the d_mat distance matrix
#use the ward method
h1 <- hclust(dist_matrix, method = "ward.D")
#Plot the dendogram from the clusters
plot(h1, hang = -1, cex = 0.4)
d_mat <- dist(data_IT[,var_names],method = "manhattan")
# Use the hclust() to generate hierarchical clusters from the d_mat distance matrix
#use the ward method
h <- hclust(d_mat, method = "ward.D")
#Plot the dendogram from the clusters
plot(h, labels = sub_IT$Symbol, hang = -1, cex = 0.4)
#Cut the tree into 5 clusters and plot these
rect.hclust(h1, k = 5, border = 1:6)
#Cut the tree into 5 clusters and plot these
rect.hclust(h1, k = 5, border = 1:5)
ct.hclust(h1, k = 5, border = 1:5)
#Cut the tree into 5 clusters and plot these
rect.hclust(h1, k = 5, border = 1:5)
#Cut the tree into 5 clusters and plot these
rect.hclust(h, k = 5, border = 2:4)
#Cut the tree into 5 clusters and plot these
rect.hclust(h1, k = 5, border = 1:5)
#Assign each company a cluster number and find out which cluster is ORCL in?
h_cluster5 <- cutree(h1, 5)
data.IT$h_cluster5 <- h_cluster5
data.IT["ORCL"]
data.IT["ORCL", ]
data.IT["ORCL", ]$h_cluster5
clusters <- cutree(h,k = 5)
clusters
table(clusters)
data_IT$clusters <- clusters
View(data_IT)
data.IT$h_cluster5 == data_IT$clusters
if(data.IT$h_cluster5 == data_IT$clusters){
"TRUE"
}
all.equal(data.IT$h_cluster5 , data_IT$clusters)
table(h_cluster5)
h_cluster5
#Summarize cluster 1 and 4 to get an idea of how they differ from each other
summary(data.IT[h_cluster5 == 1])
#Summarize cluster 1 and 4 to get an idea of how they differ from each other
summary(data.IT[, h_cluster5] == 1)
#Summarize cluster 1 and 4 to get an idea of how they differ from each other
summary(data.IT[data.IT$h_cluster5] == 1)
#Summarize cluster 1 and 4 to get an idea of how they differ from each other
summary(subset(data.IT, data.IT$h_cluster5 == 1))
#Summarize cluster 1 and 4 to get an idea of how they differ from each other
cluster1 <- subset(data.IT, data.IT$h_cluster5 == 1))
#Summarize cluster 1 and 4 to get an idea of how they differ from each other
cluster1 <- subset(data.IT, data.IT$h_cluster5 == 1)
cluster4 <- subset(data.It, data.IT$h_cluster5 == 4)
cluster4 <- subset(data.IT, data.IT$h_cluster5 == 4)
cluster
cluster1
View(cluster1)
View(cluster4)
summary(cluster1)
summary(cluster4)
summary(subset(data_IT[,var_names], data_IT$clusters == 1))
summary(subset(data_IT[,var_names], data_IT$clusters == 4))
c1<- (subset(data_IT[,var_names], data_IT$clusters == 1))
c4<- (subset(data_IT[,var_names], data_IT$clusters == 4))
View(c4)
#----[K-Means]--
# Implement k-means clustering using the standardized data for IT companies as above
#pick a number of centers
set.seed(546)
?kmeans
k1 <- kmeans(data.IT[, var_names], centers = 4)
k1 <- kmeans(data.IT[, var_names], centers = 5)
#what are the sizes of the clusters?
k1
#what are the sizes of the clusters?
k1$size
#what are the sizes of the clusters?
str(k1)
# Create the WSS curve by running kmeans for different number of clusters
wss_curve <- c()
for(i in 1:10){
k <- kmeans(data.IT[, var_names], centers = i)
wss <- k$tot.withinss
wss_curve[i] <- wss
}
plot(wss_curve)
?plot
plot(wss_curve, col = " #24477f")
plot(wss_curve, col = "#24477f")
?plot
plot(1:10, WSS_curve, type = "b", col = "red", ylab = "WSS", xlab = "K", ylim = c(0,18) )
?plot
plot(1:10, wss_curve,type ="b", col = "#24477f")
plot(1:10, wss_curve,type ="b", col = "#24477f", xlab = "Number of cluster", ylab = "wss_curve")
wss <- c()
for(i in 1:10){
k <- kmeans(data_IT[,var_names],centers = i)
wss[i] <- k$tot.withinss}
plot(1:10,y = wss,type="o",col="red", ylab = "", xlab = "K", main = "WSS")
?plot
plot(1:10, wss_curve,type ="o", col = "#24477f", xlab = "Number of cluster", ylab = "wss_curve")
plot(1:10, wss_curve,type ="o", col = "#24477f", xlab = "Number of cluster", main = "wss")
#After evaluating the Elbow plot, rerun kmeans with what you think is the most appropriate number of clusters.
k4 <- kmeans(data.It[, var_names], centers = 4)
#How many points in each cluster?
k4$size
#What are the company names of those in the smallest cluster?
k4$size ==3
#What are the company names of those in the smallest cluster?
k4
k4 <- kmeans(data.It[, var_names], centers = 4)
#After evaluating the Elbow plot, rerun kmeans with what you think is the most appropriate number of clusters.
k4 <- kmeans(data.IT[, var_names], centers = 4)
#How many points in each cluster?
k4$size
#What are the company names of those in the smallest cluster?
k4
#What are the company names of those in the smallest cluster?
str(k40
#What are the company names of those in the smallest cluster?
str(k4)
#What are the company names of those in the smallest cluster?
str(k4)
k4 <- kmeans(data_IT[,var_names], centers = 4)
k4$size
data.IT$Name[k4$cluster == 2]
View(k4)
##Plot the clustering results using the fviz_cluster()
library(fviz_cluster)
##Plot the clustering results using the fviz_cluster()
?fviz_cluster
fviz_cluster(k4, data = data.IT[, var_names], geom = "point")
fviz_cluster(k4, data=data_IT[,var_names])
fviz_cluster(k4, data = data.IT[, var_names], geom = c("point", "text")
fviz_cluster(k4, data = data.IT[, var_names], geom = c("point", "text"))
fviz_cluster(k4, data = data.IT[, var_names], geom = c("point", "text"))
View(data.IT)
View(data_states)
View(data)
#Load the data from the US_stats file
data_states = read.csv("US_States_2014.csv")
View(data_states)
View(data)
distance_matrix_norm = dist(data_norm[, 2:10], method = "euclidean")
#If you want to see the complete distance matrix
View(as.matrix(distance_matrix_norm))
