xlim (0, 30000) +
ggtitle("Linear Reg., Prediction Error vs Actual Price") +
labs(x = "Actual Price", y = "Linear Reg. Prediction Error")
#Plot of the Prediction Error vs Actual Price
p_error_lm<- ggplot(data = test_set_lr, aes(x=cnt_bike, y=lm_error)) +
geom_point(size=2, color = "seagreen") +
ylim (-5000, 8000) +
xlim (0, 30000) +
ggtitle("Linear Reg., Prediction Error vs Actual Price") +
labs(x = "Actual Price", y = "Linear Reg. Prediction Error")
grid.arrange(h_error_lm, p_error_lm)
#k) Compute the prediction error for the linear regression model and create
# a ggplot histogram for the distribute of the prediction error
#Compute Prediction error
lm_error <- lin_pred - test_set_lr$cnt_bike
#Visualize the prediction error
#Histogram of the distribution of prediction errors
h_error_lm <- ggplot(data= test_set_lr, aes(x = lm_error)) +
geom_histogram(colour = "darkgreen", fill = "seagreen") +
xlim (-10000, 10000) +
ylim (0, 150) +
ggtitle("Linear Reg., Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot of the Prediction Error vs Actual Price
p_error_lm<- ggplot(data = test_set_lr, aes(x=cnt_bike, y=lm_error)) +
geom_point(size=2, color = "seagreen") +
ylim (-5000, 8000) +
xlim (0, 30000) +
ggtitle("Linear Reg., Prediction Error vs Actual Price") +
labs(x = "Actual Price", y = "Linear Reg. Prediction Error")
grid.arrange(h_error_lm, p_error_lm)
#Visualize the prediction error
#Histogram of the distribution of prediction errors
h_error_lm <- ggplot(data= test_set_lr, aes(x = lm_error)) +
geom_histogram(colour = "darkgreen", fill = "seagreen") +
xlim (-10000, 10000) +
ylim (0, 50) +
ggtitle("Linear Reg., Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot of the Prediction Error vs Actual Price
p_error_lm<- ggplot(data = test_set_lr, aes(x=cnt_bike, y=lm_error)) +
geom_point(size=2, color = "seagreen") +
ylim (-5000, 8000) +
xlim (0, 10000) +
ggtitle("Linear Reg., Prediction Error vs Actual Price") +
labs(x = "Actual Price", y = "Linear Reg. Prediction Error")
grid.arrange(h_error_lm, p_error_lm)
#Visualize the prediction error
#Histogram of the distribution of prediction errors
h_error_lm <- ggplot(data= test_set_lr, aes(x = lm_error)) +
geom_histogram(colour = "darkgreen", fill = "seagreen") +
xlim (-5000, 5000) +
ylim (0, 50) +
ggtitle("Linear Reg., Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot of the Prediction Error vs Actual Price
p_error_lm<- ggplot(data = test_set_lr, aes(x=cnt_bike, y=lm_error)) +
geom_point(size=2, color = "seagreen") +
ylim (-5000, 8000) +
xlim (0, 10000) +
ggtitle("Linear Reg., Prediction Error vs Actual Price") +
labs(x = "Actual Price", y = "Linear Reg. Prediction Error")
grid.arrange(h_error_lm, p_error_lm)
#Visualize the prediction error
#Histogram of the distribution of prediction errors
h_error_lm <- ggplot(data= test_set_lr, aes(x = lm_error)) +
geom_histogram(colour = "darkgreen", fill = "seagreen") +
xlim (-5000, 5000) +
ylim (0, 30) +
ggtitle("Linear Reg., Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot of the Prediction Error vs Actual Price
p_error_lm<- ggplot(data = test_set_lr, aes(x=cnt_bike, y=lm_error)) +
geom_point(size=2, color = "seagreen") +
ylim (-5000, 8000) +
xlim (0, 10000) +
ggtitle("Linear Reg., Prediction Error vs Actual Price") +
labs(x = "Actual Price", y = "Linear Reg. Prediction Error")
grid.arrange(h_error_lm, p_error_lm)
######
# Product Insights
#Put together the error metrics
res_lin<- c(ME_lin, lin_MAE, linMAPE, lin_RMSE)
names(res_lin) <-c("ME", "MAE", "MAPE", "RMSE")
res_lin <- set_label(res_lin, "Linear Regression")
res_lin
######
# Product Insights
#Put together the error metrics
res_lin<- c(ME_lin, lin_MAE, linMAPE, lin_RMSE)
error_table <- c(knnME, knnRMSE, ME_tree, treeRMSE, ME_lin, lin_RMSE)
#e)
ME_lin <- mean(lm_error)
#RMSE
lin_RMSE <- RMSE(pred = lin_pred, obs = test_set_lr$cnt_bike)
error_table <- c(knnME, knnRMSE, ME_tree, treeRMSE, ME_lin, lin_RMSE)
names(error_table) <- c("KNN ME", "KNN RMSE", "TREE ME", "TREE RMSE", "LR ME", "LR RMSE")
error_table <- set_label(error_table, "Error table")
error_table
# Report the histogram for the distribution of the prediction errors
grid.arrange(h_error_knn,h_error_tree,h_error_lm, nrow = 1)
summary(df)
# histogram
ggplot(data = df, aes(cnt_bike)) +
xlim(0, 9000) +
ylim(0, 70) +
geom_histogram(colour = "grey", fill = "black") +
ggtitle("Distrubution of the number of bike-rides") +
labs(x = "Number of bikes")
# b) Use the function pairs() to produce a plot of
# the relationships among count, atemp and hum.
pairs(df[,1:3], col = "darkgreen")
View(df)
knn_model
# create a histogram of the distribution of bike rides
h_pred_knn <- ggplot(data= test_set_stand, aes(x = knnPred)) +
xlim(0, 9000) +
ylim(0, 18) +
geom_histogram(colour = "lightblue", fill = "darkblue") +
ggtitle("KNN, Distribution of Predicted bike rides") +
labs(x = "Predicted bike rides")
bike_dist<- ggplot(data=test_set_stand, aes(x = cnt_bike)) +
geom_histogram(colour = "grey", fill = "black") +
xlim (0,9000) +
ylim (0,18) +
ggtitle("Original Bike Distribution") +
labs(x = "Bike rides")
grid.arrange(bike_dist, h_pred_knn, nrow=1)
#d) Computer the prediction error for the k-NN model
# create ggolit histrogram for the error
knn_error <-knnPred - test_set_stand$cnt_bike
#Visualize the prediction error
#Histogram of the distribution of the prediction error
h_error_knn = ggplot(data= test_set_stand, aes(x = knn_error)) +
geom_histogram(colour = "lightblue", fill = "blue") +
xlim (-5000, 5000) +
ylim (0, 30) +
ggtitle("KNN, Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot prediction error vs actual price
p_error_knn<- ggplot(data = test_set_stand, aes(x=cnt_bike, y=knn_error)) +
geom_point(size=2, color = "blue") +
ylim (-5000, 8000) +
xlim (0, 10000) +
ggtitle("KNN, Prediction Error vs Actual Bike Count") +
labs(x = "Actual Bike Count", y = "KNN Prediction Error")
grid.arrange(h_error_knn, p_error_knn)
#Visualize the prediction error
#Histogram of the distribution of the prediction error
h_error_knn = ggplot(data= test_set_stand, aes(x = knn_error)) +
geom_histogram(colour = "lightblue", fill = "blue") +
xlim (-5000, 5000) +
ylim (0, 20) +
ggtitle("KNN, Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot prediction error vs actual price
p_error_knn<- ggplot(data = test_set_stand, aes(x=cnt_bike, y=knn_error)) +
geom_point(size=2, color = "blue") +
ylim (-5000, 8000) +
xlim (0, 10000) +
ggtitle("KNN, Prediction Error vs Actual Bike Count") +
labs(x = "Actual Bike Count", y = "KNN Prediction Error")
grid.arrange(h_error_knn, p_error_knn)
knnME
knnRMSE<- RMSE(pred = knnPred, obs = test_set_stand$cnt_bike)
knnRMSE
# Plot the final tree
rpart.plot(rtree$finalModel, digits=-3)
h_pred_tree<- ggplot(data= test_set, aes(x = treePred)) +
geom_histogram(colour = "red", fill = "darkred") +
xlim (0,9000) +
ylim (0, 300) +
ggtitle("Tree, Distribution of Predictions") +
labs(x = "Predictions")
#compare to the actual price distribution we created above
grid.arrange(bike_dist,h_pred_tree, nrow=1)
h_pred_tree<- ggplot(data= test_set, aes(x = treePred)) +
geom_histogram(colour = "red", fill = "darkred") +
xlim (0,9000) +
ylim (0, 100) +
ggtitle("Tree, Distribution of Predictions") +
labs(x = "Predictions")
#compare to the actual price distribution we created above
grid.arrange(bike_dist,h_pred_tree, nrow=1)
# Prediction error
tree_error <-treePred - test_set$cnt_bike
h_error_tree<- ggplot(data= test_set, aes(x = tree_error)) +
geom_histogram(colour = "darkred", fill = "red") +
xlim (-10000, 10000) +
ylim (0, 30) +
ggtitle("Tree, Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot prediction error vs actual price
p_error_tree<- ggplot(data = test_set, aes(x=cnt_bike, y=tree_error)) +
geom_point(size=2, color = "red") +
ylim (-5000, 8000) +
xlim (0, 10000) +
ggtitle("Tree, Prediction Error vs Actual Bike Count") +
labs(x = "Actual Bike Count", y = "Tree Prediction Error")
grid.arrange(h_error_tree, p_error_tree)
# i,b) Compute the ME and RMSE for the regression tree
ME_tree <- mean(tree_error)
ME_tree
ME_tree
treeRMSE
#b) Check and comment on whether using the attributes used for the prediction.
bike_dist
#compare to the actual price distribution
grid.arrange(bike_dist, h_pred_lm, nrow = 1)
grid.arrange(h_error_lm, p_error_lm)
#e)
ME_lin <- mean(lm_error)
ME_lin
lin_RMSE
error_table <- c(knnME, knnRMSE, ME_tree, treeRMSE, ME_lin, lin_RMSE)
names(error_table) <- c("KNN ME", "KNN RMSE", "TREE ME", "TREE RMSE", "LR ME", "LR RMSE")
error_table <- set_label(error_table, "Error table")
error_table
# Report the histogram for the distribution of the prediction errors
grid.arrange(h_error_knn,h_error_tree,h_error_lm, nrow = 1)
#Visualize the prediction error
#Histogram of the distribution of the prediction error
h_error_knn = ggplot(data= test_set_stand, aes(x = knn_error)) +
geom_histogram(colour = "lightblue", fill = "blue") +
xlim (-5000, 5000) +
ylim (0, 30) +
ggtitle("KNN, Distribution of Prediction Error") +
labs(x = "Prediction Error")
# Report the histogram for the distribution of the prediction errors
grid.arrange(h_error_knn,h_error_tree,h_error_lm, nrow = 1)
# Analysis
# Processing and Visualizing data
# a) Load the data. Get a summary of the data, report it. Use ggplot to plot a histogram
# for the distribution of the number of bike-rides.
df <- read.csv("bike_day.csv")
summary(df)
# histogram
ggplot(data = df, aes(cnt_bike)) +
xlim(0, 9000) +
ylim(0, 70) +
geom_histogram(colour = "grey", fill = "black") +
ggtitle("Distrubution of the number of bike-rides") +
labs(x = "Number of bikes")
# b) Use the function pairs() to produce a plot of
# the relationships among count, atemp and hum.
pairs(df[,1:3], col = "darkgreen")
# c) (0.2) Split the data into 80% training and 20% testing.
trainRows <- createDataPartition(y = df$cnt_bike, p = 0.8, list = FALSE)
train_set <- df[trainRows,]
test_set <- df[-trainRows,]
train_set_stand <- train_set
test_set_stand <- test_set
library(standardize)
#Apply the standardization
train_set_stand[,2:7] <- apply(train_set_stand[,2:7], MARGIN = 2, FUN = scale)
test_set_stand[,2:7]<- apply(test_set_stand[,2:7], MARGIN = 2, FUN = scale)
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
# c) Get the predictions from k-NN model
knnPred <- predict(knn_model, test_set_stand)
# create a histogram of the distribution of bike rides
h_pred_knn <- ggplot(data= test_set_stand, aes(x = knnPred)) +
xlim(0, 9000) +
ylim(0, 18) +
geom_histogram(colour = "lightblue", fill = "darkblue") +
ggtitle("KNN, Distribution of Predicted bike rides") +
labs(x = "Predicted bike rides")
bike_dist<- ggplot(data=test_set_stand, aes(x = cnt_bike)) +
geom_histogram(colour = "grey", fill = "black") +
xlim (0,9000) +
ylim (0,18) +
ggtitle("Original Bike Distribution") +
labs(x = "Bike rides")
grid.arrange(bike_dist, h_pred_knn, nrow=1)
#d) Computer the prediction error for the k-NN model
# create ggolit histrogram for the error
knn_error <-knnPred - test_set_stand$cnt_bike
#Visualize the prediction error
#Histogram of the distribution of the prediction error
h_error_knn = ggplot(data= test_set_stand, aes(x = knn_error)) +
geom_histogram(colour = "lightblue", fill = "blue") +
xlim (-5000, 5000) +
ylim (0, 30) +
ggtitle("KNN, Distribution of Prediction Error") +
labs(x = "Prediction Error")
#Plot prediction error vs actual price
p_error_knn<- ggplot(data = test_set_stand, aes(x=cnt_bike, y=knn_error)) +
geom_point(size=2, color = "blue") +
ylim (-5000, 8000) +
xlim (0, 10000) +
ggtitle("KNN, Prediction Error vs Actual Bike Count") +
labs(x = "Actual Bike Count", y = "KNN Prediction Error")
grid.arrange(h_error_knn, p_error_knn)
#e)
knnME <- mean(knn_error)
knnME
knnRMSE<- RMSE(pred = knnPred, obs = test_set_stand$cnt_bike)
knnRMSE
# or the linear Regression model. I would not suggest the company
# to use RegressionTree model because even though the RMSE is a bit
# lower than the Linear Regression model, its ME is -178, which means
# it is constantly underestimating.
# I would suggest the K-NN model because even though its ME is 99, its
# RMSE is the lowest. Lowest RMSE means that it produces least error
# overall on average.
# Also, I would suggest linear Regression model because it has the lowest
# ME. It has ME of -56 which means that on average it only underestimate
set.seed(123)
# import packages
library(GGally)
library(caret)
library(rpart.plot)
library(gridExtra)
library(labelVector)
library(tidyverse)
# Analysis
# Processing and Visualizing data
# a) Load the data. Get a summary of the data, report it. Use ggplot to plot a histogram
# for the distribution of the number of bike-rides.
df <- read.csv("bike_day.csv")
summary(df)
# histogram
ggplot(data = df, aes(cnt_bike)) +
xlim(0, 9000) +
ylim(0, 70) +
geom_histogram(colour = "grey", fill = "black") +
ggtitle("Distrubution of the number of bike-rides") +
labs(x = "Number of bikes")
# b) Use the function pairs() to produce a plot of
# the relationships among count, atemp and hum.
pairs(df[,1:3], col = "darkgreen")
# c) (0.2) Split the data into 80% training and 20% testing.
trainRows <- createDataPartition(y = df$cnt_bike, p = 0.8, list = FALSE)
train_set <- df[trainRows,]
test_set <- df[-trainRows,]
train_set_stand <- train_set
test_set_stand <- test_set
library(standardize)
#Apply the standardization
train_set_stand[,2:7] <- apply(train_set_stand[,2:7], MARGIN = 2, FUN = scale)
test_set_stand[,2:7]<- apply(test_set_stand[,2:7], MARGIN = 2, FUN = scale)
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
# c) Get the predictions from k-NN model
knnPred <- predict(knn_model, test_set_stand)
# create a histogram of the distribution of bike rides
h_pred_knn <- ggplot(data= test_set_stand, aes(x = knnPred)) +
xlim(0, 9000) +
ylim(0, 18) +
geom_histogram(colour = "lightblue", fill = "darkblue") +
ggtitle("KNN, Distribution of Predicted bike rides") +
labs(x = "Predicted bike rides")
View(test_set_stand)
View(train_set_stand)
# c) (0.2) Split the data into 80% training and 20% testing.
trainRows <- createDataPartition(y = df$cnt_bike, p = 0.8, list = FALSE)
train_set <- df[trainRows,]
test_set <- df[-trainRows,]
train_set_stand <- train_set
test_set_stand <- test_set
library(standardize)
#Apply the standardization
train_set_stand[,2:7] <- apply(train_set_stand[,2:7], MARGIN = 2, FUN = scale)
test_set_stand[,2:7]<- apply(test_set_stand[,2:7], MARGIN = 2, FUN = scale)
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
# c) Get the predictions from k-NN model
knnPred <- predict(knn_model, test_set_stand)
View(test_set_stand)
View(train_set_stand)
# c) Get the predictions from k-NN model
knnPred <- predict(knn_model, test_set_stand)
View(knn_model)
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
# import packages
library(reprex)
library(reprex)
library(GGally)
library(caret)
library(rpart.plot)
library(gridExtra)
library(labelVector)
library(tidyverse)
library(tidyverse)
# Analysis
# Processing and Visualizing data
# a) Load the data. Get a summary of the data, report it. Use ggplot to plot a histogram
# for the distribution of the number of bike-rides.
df <- read.csv("bike_day.csv")
summary(df)
# histogram
ggplot(data = df, aes(cnt_bike)) +
xlim(0, 9000) +
ylim(0, 70) +
geom_histogram(colour = "grey", fill = "black") +
ggtitle("Distrubution of the number of bike-rides") +
labs(x = "Number of bikes")
reprex()
reprex()
reprex()
str(df)
# histogram
ggplot(df, aes(cnt_bike)) +
xlim(0, 9000) +
ylim(0, 70) +
geom_histogram(colour = "grey", fill = "black") +
ggtitle("Distrubution of the number of bike-rides") +
labs(x = "Number of bikes")
reprex()
library(tidyverse)
install.packages("tidyverse")
version("tidyverse")
# import packages
library(GGally)
library(caret)
library(rpart.plot)
library(gridExtra)
library(labelVector)
library(tidyverse)
reprex()
library(reprex)
reprex()
reprex:::reprex_addin()
reprex:::reprex_addin()
render(hw5.R)
library(mmarkdown)
library(markdown)
render(hw5.R)
render("hw5.R")
render("hw5R.Rmd")
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("bike_day.csv")
summary(df)
ggplot(data = df, aes(cnt_bike)) +
xlim(0, 9000) +
ylim(0, 70) +
geom_histogram(colour = "grey", fill = "black") +
ggtitle("Distrubution of the number of bike-rides") +
labs(x = "Number of bikes")
pairs(df[,1:3], col = "darkgreen")
trainRows <- createDataPartition(y = df$cnt_bike, p = 0.8, list = FALSE)
train_set <- df[trainRows,]
test_set <- df[-trainRows,]
train_set_stand <- train_set
test_set_stand <- test_set
library(standardize)
#Apply the standardization
train_set_stand[,2:7] <- apply(train_set_stand[,2:7], MARGIN = 2, FUN = scale)
test_set_stand[,2:7] <- apply(test_set_stand[,2:7], MARGIN = 2, FUN = scale)
set.seed(123)
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
# import packages
library(GGally)
library(caret)
library(rpart.plot)
library(gridExtra)
library(labelVector)
library(tidyverse)
df <- read.csv("bike_day.csv")
summary(df)
ggplot(data = df, aes(cnt_bike)) +
xlim(0, 9000) +
ylim(0, 70) +
geom_histogram(colour = "grey", fill = "black") +
ggtitle("Distrubution of the number of bike-rides") +
labs(x = "Number of bikes")
pairs(df[,1:3], col = "darkgreen")
trainRows <- createDataPartition(y = df$cnt_bike, p = 0.8, list = FALSE)
train_set <- df[trainRows,]
test_set <- df[-trainRows,]
train_set_stand <- train_set
test_set_stand <- test_set
library(standardize)
#Apply the standardization
train_set_stand[,2:7] <- apply(train_set_stand[,2:7], MARGIN = 2, FUN = scale)
test_set_stand[,2:7] <- apply(test_set_stand[,2:7], MARGIN = 2, FUN = scale)
knn_model <- train(cnt_bike~., train_set_stand, method = "knn")
knn_model
( (0.996*0.993) + (0.087*0.12) + (0.17*0))
( sqrt( 0.996^2 + 0.087^2 + 0.17^2) * sqrt( 0.993^2 + 0.12^2 + 0^2) )
0.999468 / 1.01437
( (0.996*0.847) + (0.087*0.466) + (0.17*0.254)) / ( sqrt( 0.996^2 + 0.087^2 + 0.17^2) * sqrt( 0.847^2 + 0.466^2 + 0^254) )
( (0.996*0.847) + (0.087*0.466) + (0.17*0.254))
( sqrt( 0.996^2 + 0.087^2 + 0.17^2) * sqrt( 0.847^2 + 0.466^2 + 0^254) )
0.927334 / 0.980401
c((0.996 - 0.993), (0.087 - 0.12), (0.17- 0))
a <- c((0.996 - 0.993), (0.087 - 0.12), (0.17- 0))
abs(a)
max(a)
b <- c((0.996 - 0.847), (0.087 - 0.466), (0.17 - 0.254))
b
max(abs(b))
( (0.996*0.993) + (0.087*0.12) + (0.17*0)) / ( sqrt( 0.996^2 + 0.087^2 + 0.17^2) * sqrt( 0.993^2 + 0.12^2 + 0^2) )
( (0.996*0.847) + (0.087*0.466) + (0.17*0.254)) / ( sqrt( 0.996^2 + 0.087^2 + 0.17^2) * sqrt( 0.847^2 + 0.466^2 + 0^254) )
